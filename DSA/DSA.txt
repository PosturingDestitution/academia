DATA STRUCTURES

why would you use a data structure? if you need to organize data effectively, optimized searching, sorting, and retreaval, reduces redundancy and optimized space

1. linear
	array - fixed-size colleciton of elements stored in contiguous memory
	linked list - a collection of nodes where each node contains data and a pointer to the next node
	stack - LIFO used for operations like undo mechanisms
	queue - FIFO used for scheduling tasks

2. nonlinear

	tree - hierarchical structure with nodes connected by edges
	graph - collection of nodes (verticies) connected by edges, used to model networks and realtionships
	heap - a specialized tree-based structure used in priority queues

3. static data structures
	fixed size and memory allocation (array)

4. dynamic data structures
	can grow or shrink at runtime (linked list, hash tables)

5. hierarchical data structures
	follows a parent-child relationship (trees and heaps)

6. hashing-based data structure

	hash table - stores key-value pairs, allowing effcient searching and retrieval
	set - stores unique elements and allows fast membership checking
	
ALGORITHMS

An Algorithm is a step-by-step procedure or set of rules to solve a problem or perform a computation.

The types of algorithms:
1. search (binary, linear)
2. sorting (merge, quick, bubble)
3. divide and conquer (merge, quick)
4. greedy algoitthms (Kruskal's, Prim's)
5. DP (fibonacci series, knapsack)
6. backtracking algoithms (N-Queens, maze solving)
7. graph algorithms (Dijkstra's, BFS, DFS)
8. brute force algorthms (password cracking, pattern matching)

TIME COMPLEXITY

Time Complexity is the amount of time an algoithm takes to run as a function of the input size (n)

steps to analyze time complexity
1. identify the basic operations ...like those that take constant time (comparisons, assignments, or arthmetic)
2. count the number of executions ... for loops, nested loops, and recursive calls are the key area to focus on
3. analyze control structures  .. for loops: O(n), nested loops O(n^2) or more depending on nesting, divide annd conquer O(log n) or O(n log n)
4. ignore constants and lower order terms ... drop constant factors like 2n becomes O(n) ... drop lower order terms like n^2 + n becomes O(n^2)
5. check for recursive calls (use recurrence relations) ... example: merge sort T(n) = 2T(n/2) + O(n) -> solves to O(n log n)

common time complexities
O(1) accessing an element in an array (constant time)
O(log n) binary search (halving the input size)
O(n) linear seach, single loop (one pass through input)
O(n log n) merge sort, quick sort (divide and conquer)
O(2^n) recursive backtracking (exponential growth)
O(n!) traveling salesman problem (factorial growth)

SPACIAL COMPLEXITY

"Space complexity refers to the amount of memory or space an algorithm uses as a function of the size of its input. It measures the growth of memory usage with respect to the input size, helping you understand how efficient an algorithm is in terms of memory."

steps to determine spacial complexity
1. identify the input variables
2. analyze the data structures
3. consider recursion (stack space)
4. look at extra memory usage